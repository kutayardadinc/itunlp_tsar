{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b550058f",
   "metadata": {},
   "source": [
    "# Simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61b3279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=\"your-api-key-here\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab1d5ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_llm_output(raw_output: str) -> str:\n",
    "    \"\"\"\n",
    "    Cleans the raw output from the language model by removing unwanted formatting.\n",
    "    \"\"\"\n",
    "    \n",
    "    if \"---\" in raw_output:\n",
    "        raw_output = raw_output.split(\"---\")[0].strip()\n",
    "\n",
    "    lines = raw_output.splitlines()\n",
    "\n",
    "    if lines:\n",
    "        lines[0] = lines[0].replace(\"*\", \"\").strip()\n",
    "\n",
    "    cleaned = \"\\n\".join(\n",
    "        \" \".join(line.strip().split())\n",
    "        for line in lines\n",
    "        if line.strip()\n",
    "    )\n",
    "\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab603dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversion_config = { # Configuration to be used in prompts\n",
    "  \"model\": \"gpt-4o-2024-11-20\",\n",
    "  \"temperature\": 0.5,\n",
    "  \"top_p\": 0.95,\n",
    "  \"max_tokens\": 8192\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56080152",
   "metadata": {},
   "source": [
    "## Syntactic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7294e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"grammar/grammar_rules.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "        grammar_rules = json.load(file) # Load grammar rules from JSON file for syntactic simplification\n",
    "\n",
    "level_order = [\"A1\", \"A2\", \"B1\"]\n",
    "\n",
    "def load_grammar_rules(level):\n",
    "    return grammar_rules.get(level, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8c9b31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"rules/simplification_rules_en.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    simplification_rules = json.load(file) # Load syntactic simplification rules from JSON file\n",
    "\n",
    "def load_simplification_rules(simplification_type, level):\n",
    "    return simplification_rules[level][simplification_type].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d840f962",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_syntactic(original_text, level, type=\"syntactic\"):\n",
    "    \n",
    "    system_instruction = \"You are an expert in syntactically simplifying sentences. Your task is to identify grammatical rules and morphemes in the given text that fall outside the specified language level and to simplify the text to this level using syntactic simplification rules.\"\n",
    "    chat_history = [{\"role\": \"system\", \"content\": system_instruction}]\n",
    "\n",
    "    simplification_rules = load_simplification_rules(type, level) # Load syntactic simplification rules\n",
    "    grammar_rules = load_grammar_rules(level)\n",
    "    \n",
    "    prompt = (\n",
    "            f\"A text at the {level} level should not contain any structure other than the grammatical rules and morphemes given to you.\\n\"\n",
    "            f\"If the given text contains any structure other than these grammatical rules and morphemes, simplify the given text to the {level} level using syntactic simplification rules.\\n\\n---\\n\\n\"\n",
    "            f\"The grammatical rules and morphemes for the {level} level are as follows:\\n\"\n",
    "            f\"- \" + \"\\n- \".join(grammar_rules) + \"\\n\\n---\\n\\n\"\n",
    "            f\"{simplification_rules}\\n\\n---\\n\\n\"\n",
    "            f\"Do not perform any action other than syntactic simplification. Preserve the content and details of the text. Do not delete sentences from the text. In the output, provide only the simplified text, do not write any explanation.\\n\\n---\\n\\n\"\n",
    "            f\"Original Text:\\n{original_text}\\n\"\n",
    "            \"Syntactically Simplified Text:\\n\")\n",
    "\n",
    "    chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    # Call the OpenAI API\n",
    "    response = client.chat.completions.create(\n",
    "        model=conversion_config[\"model\"],\n",
    "        messages=chat_history,\n",
    "        temperature=conversion_config[\"temperature\"],\n",
    "        top_p=conversion_config[\"top_p\"],\n",
    "        max_tokens=conversion_config[\"max_tokens\"],\n",
    "    )\n",
    "    \n",
    "    output = response.choices[0].message.content\n",
    "    cleaned_output = clean_llm_output(output)\n",
    "    return cleaned_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14c7296",
   "metadata": {},
   "source": [
    "## Lexical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2afb72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Load English language model for tokenization and lemmatization\n",
    "\n",
    "\n",
    "def load_word_level_data(json_path=\"labeled_words/all_words.json\"):\n",
    "    \"\"\"Load CEFR word level mappings from JSON file.\"\"\"\n",
    "    with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        levels_dict = json.load(f)\n",
    "\n",
    "    word_to_level_map = {}\n",
    "    for level, words in levels_dict.items():\n",
    "        for word in words:\n",
    "            word_to_level_map[word.lower()] = level\n",
    "            \n",
    "    return word_to_level_map\n",
    "\n",
    "\n",
    "def get_sentence_levels(sentence, word_map):\n",
    "    \"\"\"Analyze sentence and return CEFR level for each word.\"\"\"\n",
    "    if not word_map:\n",
    "        print(\"Word map is empty. Cannot determine levels.\")\n",
    "        return {}\n",
    "        \n",
    "    doc = nlp(sentence)\n",
    "    level_results = {}\n",
    "\n",
    "    for token in doc:\n",
    "        if token.is_punct:\n",
    "            continue\n",
    "\n",
    "        original_word = token.text\n",
    "        word_text_lower = token.text.lower()\n",
    "        word_lemma_lower = token.lemma_.lower()\n",
    "        \n",
    "        found_level = None\n",
    "\n",
    "        # Check both original form and lemma for level matching\n",
    "        if word_text_lower in word_map:\n",
    "            found_level = word_map[word_text_lower]\n",
    "        elif word_lemma_lower in word_map:\n",
    "            found_level = word_map[word_lemma_lower]\n",
    "        \n",
    "        level_results[original_word] = found_level\n",
    "        \n",
    "    return level_results\n",
    "\n",
    "\n",
    "# Load word level data for lexical analysis\n",
    "JSON_FILE_PATH = 'labeled_words/all_words.json'\n",
    "word_level_map = load_word_level_data(JSON_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f08637b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_above_words(level: str, word_levels: dict) -> list:\n",
    "    \"\"\"Find words that are above the specified CEFR level.\"\"\"\n",
    "    \n",
    "    levels = [\"A1\", \"A2\", \"B1\", \"B2\", \"C1\"]\n",
    "    \n",
    "    min_index = levels.index(level) + 1\n",
    "    above_levels = set(levels[min_index:])\n",
    "\n",
    "    return [word for word, lvl in word_levels.items() if lvl in above_levels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65c993bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_lexical(original_text, level, type=\"lexical\"):\n",
    "    system_instruction = \"You are an expert in lexically simplifying sentences. Your task is to simplify the given text to the specified level using lexical simplification rules.\"\n",
    "    chat_history = [{\"role\": \"system\", \"content\": system_instruction}]\n",
    "\n",
    "    simplification_rules = load_simplification_rules(type, level) # Load lexical simplification rules\n",
    "    \n",
    "    word_levels = get_sentence_levels(original_text, word_level_map) # CEFR level for each word in the text\n",
    "    words_above_prompt = \"\"\n",
    "    words_above = find_above_words(level, word_levels) # Words above the target level\n",
    "    \n",
    "    if words_above:\n",
    "        if len(words_above) == 1:\n",
    "            words_above_prompt = f\"In the following text, the word '{words_above[0]}' is above the {level} level. Without changing the meaning of the text, replace this word with a simpler word appropriate for the {level} level. If there is no simpler equivalent for this word or if it is important for the meaning of the text, do not change the word.\"\n",
    "        elif len(words_above) == 2:\n",
    "            words_above_prompt = f\"In the following text, the words '{' and '.join(words_above)}' are above the {level} level. Without changing the meaning of the text, replace these words with simpler words appropriate for the {level} level. Do not change any words that have no simpler equivalent or are important for the meaning of the text.\"\n",
    "        else:\n",
    "            words_above_prompt = f\"In the following text, the words {', '.join(words_above[:-1])}, and {words_above[-1]} are above the {level} level. Without changing the meaning of the text, replace these words with simpler words appropriate for the {level} level. Do not change any words that have no simpler equivalent or are important for the meaning of the text.\"\n",
    "        \n",
    "    prompt = (\n",
    "            f\"Simplify the given text to the {level} level using lexical simplification rules.\\n\\n---\\n\\n\"\n",
    "            f\"{simplification_rules}\\n\\n---\\n\\n\"\n",
    "            f\"{words_above_prompt}\\n\"\n",
    "            f\"Do not perform any action other than lexical simplification. Preserve the content and details of the text. Do not delete sentences from the text. In the output, provide only the simplified text, do not write any explanation.\\n\\n---\\n\\n\"\n",
    "            f\"Original Text:\\n{original_text}\\n\"\n",
    "            \"Lexically Simplified Text:\\n\")\n",
    "\n",
    "    chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=conversion_config[\"model\"],\n",
    "        messages=chat_history,\n",
    "        temperature=conversion_config[\"temperature\"],\n",
    "        top_p=conversion_config[\"top_p\"],\n",
    "        max_tokens=conversion_config[\"max_tokens\"],\n",
    "    )\n",
    "    \n",
    "    output = response.choices[0].message.content\n",
    "    cleaned_output = clean_llm_output(output)\n",
    "    return cleaned_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8068bf94",
   "metadata": {},
   "source": [
    "## Elaboration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "93de2e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify_elaboration(original_text, level, type=\"elaboration\"):\n",
    "    system_instruction = \"You are an expert in making sentences more understandable through the method of elaboration. Your task is to use elaboration rules to make the given text more understandable for a student at the specified level.\"\n",
    "    chat_history = [{\"role\": \"system\", \"content\": system_instruction}]\n",
    "\n",
    "    simplification_rules = load_simplification_rules(type, level) # Load elaboration rules\n",
    "        \n",
    "    prompt = (\n",
    "            f\"Using elaboration rules, make the given text more understandable for a student at the {level} level.\\n\\n---\\n\\n\"\n",
    "            f\"{simplification_rules}\\n\\n---\\n\\n\"\n",
    "            f\"Do not perform any action other than elaboration. Preserve the content and details of the text. Do not delete sentences from the text. In the output, provide only the elaborated text, do not write any explanation.\\n\\n---\\n\\n\"\n",
    "            f\"Original Text:\\n{original_text}\\n\"\n",
    "            \"Elaborated Text:\\n\")\n",
    "\n",
    "    chat_history.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=conversion_config[\"model\"],\n",
    "        messages=chat_history,\n",
    "        temperature=conversion_config[\"temperature\"],\n",
    "        top_p=conversion_config[\"top_p\"],\n",
    "        max_tokens=conversion_config[\"max_tokens\"],\n",
    "    )\n",
    "    \n",
    "    output = response.choices[0].message.content\n",
    "    cleaned_output = clean_llm_output(output)\n",
    "    return cleaned_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "267e6260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def simplify_and_save(input_file_path, output_file_path):\n",
    "    output_dir = os.path.dirname(output_file_path)\n",
    "    \n",
    "    if output_dir:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "    with open(input_file_path, 'r', encoding='utf-8') as infile, \\\n",
    "            open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "        \n",
    "        for line in infile:\n",
    "            data = json.loads(line)\n",
    "            \n",
    "            original_text = data.get(\"original\")\n",
    "            target_level = data.get(\"target_cefr\")\n",
    "            text_id = data.get(\"text_id\")\n",
    "\n",
    "            if original_text and target_level and text_id:\n",
    "                print(f\"--- Processing text_id: {text_id} ---\")\n",
    "                \n",
    "                # Syntactic Simplification\n",
    "                syntactic = clean_llm_output(simplify_syntactic(original_text, target_level))\n",
    "                \n",
    "                # Lexical Simplification\n",
    "                lexical = clean_llm_output(simplify_lexical(syntactic, target_level))\n",
    "                \n",
    "                # Elaboration\n",
    "                final_simplification = clean_llm_output(simplify_elaboration(lexical, target_level))\n",
    "                \n",
    "                output_data = {\n",
    "                    \"text_id\": text_id,\n",
    "                    \"simplified\": final_simplification\n",
    "                }\n",
    "                \n",
    "                outfile.write(json.dumps(output_data) + '\\n')\n",
    "                \n",
    "    print(f\"\\nProcessing complete. Output saved to '{output_file_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc9ea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = 'input_file.jsonl'\n",
    "output_file = 'output_file.jsonl'\n",
    "\n",
    "simplify_and_save(input_file, output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
